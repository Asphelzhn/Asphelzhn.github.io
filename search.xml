<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo Next主题美化]]></title>
    <url>%2F2018%2F12%2F19%2Fhexo%2F</url>
    <content type="text"><![CDATA[Hexo Next主题美化Hexo支持很多自定义主题和插件，本人使用的是Next主题，也涉及很多美化，为此记录一下。 添加页脚访客人数和总访问量使用的是不蒜子来进行统计,不蒜子是一款记录访客和访问量的插件. 1.安装脚本要使用不蒜子必须在页面中引入busuanzi.js，在themes/next/layout/_partial/footer.swig中添加脚本，代码如下 1&lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 2.安装标签要显示站点总访问量，复制以下代码添加到你需要显示的位置。有两种算法可选： 算法a：pv的方式，单个用户连续点击n篇文章，记录n次访问量。 123&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt; 本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt; 算法b：uv的方式，单个用户连续点击n篇文章，只记录1次访客数。123&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/span&gt; 3.安装步骤一般显示站点访问量在页脚，所以在footer.swig中添加标签，在themes/next/_config.yml中加入以下配置： 12# visitors count counter: true 在themes/next/layout/_partial/footer.swig中添加以下代码： 123456&#123;% if theme.footer.counter %&#125; &lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人&lt;/span&gt;&#123;% endif %&#125; 这样便可以在底部显示访问量了。之后使用hexo clean清空缓存，使用hexo generate重新生成站点文件，使用hexo deploy部署，就能看到效果了。 设置Menu菜单栏会显示可以跳转的页面。 如果还要添加，编辑themes/next/_config.yml： 12345678menu: home: / || home //首页 about: /about/ || user //关于 tags: /tags/ || tags //标签 categories: /categories/ || th //分类 archives: /archives/ || archive //归档 schedule: /schedule/ || calendar //日程表 sitemap: /sitemap.xml || sitemap //站点地图 将需要的Menu前面#号去掉。 设置动态背景主题配置文件中找到canvas_nest，设置成ture 12# Canvas-nestcanvas_nest: ture 修改底部标签样式 修改Blog\themes\next\layout\_macro\post.swig中文件，搜索rel=&quot;tag&quot;&gt;#，将#替换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。 侧边栏社交图标设置 打开主题配置文件_config.yml，搜索Social，社交账号前面的#号去掉。 12345678910111213#social: GitHub: https://github.com/yourname || github 简书: https://www.jianshu.com/u/63445e24e8bf || heartbeat 掘金: https://juejin.im/user/5a371ae551882512d0607108 || spinner #E-Mail: mailto:yourname@gmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype 添加网页顶部进度加载条 编辑主题配置文件，搜索pace，将其值改为ture就可以了，选择一款你喜欢的样式。 12345678910111213141516171819# Progress bar in the top during page loading.pace: ture# Themes list:#pace-theme-big-counter#pace-theme-bounce#pace-theme-barber-shop#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar#pace-theme-mac-osx#pace-theme-minimal# For example# pace_theme: pace-theme-center-simplepace_theme: pace-theme-minimal 添加点击爱心效果 在/themes/next/source/js/src下新建文件 clicklove.js ，接着把代码拷贝粘贴到 clicklove.js 文件中。 1!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 在\themes\next\layout\_layout.swig文件末尾添加： 1&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>博客 - Hexo - 记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python字符串常用方法]]></title>
    <url>%2F2018%2F12%2F19%2Fpython%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[1.字符串的常用操作1) 判断类型 - 9 方法 说明 string.isspace() 如果 string 中只包含空格，则返回 True string.isalnum() 如果 string 至少有一个字符并且所有字符都是字母或数字则返回 True string.isalpha() 如果 string 至少有一个字符并且所有字符都是字母则返回 True string.isdecimal() 如果 string 只包含数字则返回 True，全角数字，（常用） string.isdigit() 如果 string 只包含数字则返回 True，全角数字、⑴、\u00b2 string.isnumeric() 如果 string 只包含数字则返回 True，全角数字，汉字数字 string.istitle() 如果 string 是标题化的(每个单词的首字母大写)则返回 True string.islower() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True string.isupper() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True 2) 查找和替换 - 7 方法 说明 string.startswith(str) 检查字符串是否是以 str 开头，是则返回 True string.endswith(str) 检查字符串是否是以 str 结束，是则返回 True string.find(str, start=0, end=len(string)) 检测 str 是否包含在 string 中，如果 start 和 end 指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回 -1 string.rfind(str, start=0, end=len(string)) 类似于 find()，不过是从右边开始查找 string.index(str, start=0, end=len(string)) 跟 find() 方法类似，不过如果 str 不在 string 会报错 string.rindex(str, start=0, end=len(string)) 类似于 index()，不过是从右边开始 string.replace(old_str, new_str, num=string.count(old)) 把 string 中的 old_str 替换成 new_str，如果 num 指定，则替换不超过 num 次 3) 大小写转换 - 5 方法 说明 string.capitalize() 把字符串的第一个字符大写 string.title() 把字符串的每个单词首字母大写 string.lower() 转换 string 中所有大写字符为小写 string.upper() 转换 string 中的小写字母为大写 string.swapcase() 翻转 string 中的大小写 4) 文本对齐 - 3 方法 说明 string.ljust(width) 返回一个原字符串左对齐，并使用空格填充至长度 width 的新字符串 string.rjust(width) 返回一个原字符串右对齐，并使用空格填充至长度 width 的新字符串 string.center(width) 返回一个原字符串居中，并使用空格填充至长度 width 的新字符串 5) 去除空白字符 - 3 方法 说明 string.lstrip() 截掉 string 左边（开始）的空白字符 string.rstrip() 截掉 string 右边（末尾）的空白字符 string.strip() 截掉 string 左右两边的空白字符 6) 拆分和连接 - 5 方法 说明 string.partition(str) 把字符串 string 分成一个 3 元素的元组 (str前面, str, str后面) string.rpartition(str) 类似于 partition() 方法，不过是从右边开始查找 string.split(str=””, num) 以 str 为分隔符拆分 string，如果 num 有指定值，则仅分隔 num + 1 个子字符串，str 默认包含 ‘\r’, ‘\t’, ‘\n’ 和空格 string.splitlines() 按照行(‘\r’, ‘\n’, ‘\r\n’)分隔，返回一个包含各行作为元素的列表 string.join(seq) 以 string 作为分隔符，将 seq 中所有的元素（的字符串表示）合并为一个新的字符串 2.字符串的切片 切片 方法适用于 字符串、列表、元组 切片 使用 索引值 来限定范围，从一个大的 字符串 中 切出 小的 字符串 列表 和 元组 都是 有序 的集合，都能够 通过索引值 获取到对应的数据 字典 是一个 无序 的集合，是使用 键值对 保存数据 1字符串[开始索引:结束索引:步长] 注意： 指定的区间属于 左闭右开 型 [开始索引, 结束索引) =&gt; 开始索引 &gt;= 范围 &lt; 结束索引 从 起始 位开始，到 结束位的前一位 结束（不包含结束位本身) 从头开始，开始索引 数字可以省略，冒号不能省略 到末尾结束，结束索引 数字可以省略，冒号不能省略 步长默认为 1，如果连续切片，数字和冒号都可以省略 索引的顺序和倒序 在 Python 中不仅支持 顺序索引，同时还支持 倒序索引 所谓倒序索引就是 从右向左 计算索引 最右边的索引值是 -1，依次递减 演练需求 截取从 2 ~ 5 位置 的字符串 截取从 2 ~ 末尾 的字符串 截取从 开始 ~ 5 位置 的字符串 截取完整的字符串 从开始位置，每隔一个字符截取字符串 从索引 1 开始，每隔一个取一个 截取从 2 ~ 末尾 - 1 的字符串 截取字符串末尾两个字符 字符串的逆序（面试题） 答案 1234567891011121314151617181920212223242526272829303132num_str = &quot;0123456789&quot;# 1. 截取从 2 ~ 5 位置 的字符串print(num_str[2:6])# 2. 截取从 2 ~ `末尾` 的字符串print(num_str[2:])# 3. 截取从 `开始` ~ 5 位置 的字符串print(num_str[:6])# 4. 截取完整的字符串print(num_str[:])# 5. 从开始位置，每隔一个字符截取字符串print(num_str[::2])# 6. 从索引 1 开始，每隔一个取一个print(num_str[1::2])# 倒序切片# -1 表示倒数第一个字符print(num_str[-1])# 7. 截取从 2 ~ `末尾 - 1` 的字符串print(num_str[2:-1])# 8. 截取字符串末尾两个字符print(num_str[-2:])# 9. 字符串的逆序（面试题）print(num_str[::-1]) 公共方法1.Python 内置函数Python 包含了以下内置函数： 函数 描述 备注 len(item) 计算容器中元素个数 del(item) 删除变量 del 有两种方式 max(item) 返回容器中元素最大值 如果是字典，只针对 key 比较 min(item) 返回容器中元素最小值 如果是字典，只针对 key 比较 cmp(item1, item2) 比较两个值，-1 小于/0 相等/1 大于 Python 3.x 取消了 cmp 函数 注意 字符串 比较符合以下规则： “0” &lt; “A” &lt; “a” 2.切片 描述 Python 表达式 结果 支持的数据类型 切片 “0123456789”[::-2] “97531” 字符串、列表、元组 切片 使用 索引值 来限定范围，从一个大的 字符串 中 切出 小的 字符串 列表 和 元组 都是 有序 的集合，都能够 通过索引值 获取到对应的数据 字典 是一个 无序 的集合，是使用 键值对 保存数据 3.运算符 运算符 Python 表达式 结果 描述 支持的数据类型 + [1, 2] + [3, 4] [1, 2, 3, 4] 合并 字符串、列表、元组 * [“Hi!”] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 重复 字符串、列表、元组 in 3 in (1, 2, 3) True 元素是否存在 字符串、列表、元组、字典 not in 4 not in (1, 2, 3) True 元素是否不存在 字符串、列表、元组、字典 &gt; &gt;= == &lt; &lt;= (1, 2, 3) &lt; (2, 2, 3) True 元素比较 字符串、列表、元组 注意 in 在对 字典 操作时，判断的是 字典的键 in 和 not in 被称为 成员运算符 成员运算符成员运算符用于 测试 序列中是否包含指定的 成员 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False 3 in (1, 2, 3) 返回 True not in 如果在指定的序列中没有找到值返回 True，否则返回 False 3 not in (1, 2, 3) 返回 False 注意：在对 字典 操作时，判断的是 字典的键 4.完整的 for 循环语法 在 Python 中完整的 for 循环 的语法如下： 12345for 变量 in 集合: 循环体代码else: 没有通过 break 退出循环，循环结束后，会执行的代码 应用场景 在 迭代遍历 嵌套的数据类型时，例如 一个列表包含了多个字典 需求：要判断 某一个字典中 是否存在 指定的 值 如果 存在，提示并且退出循环 如果 不存在，在 循环整体结束 后，希望 得到一个统一的提示 123456789101112131415161718192021222324252627282930students = [ &#123;"name": "阿土", "age": 20, "gender": True, "height": 1.7, "weight": 75.0&#125;, &#123;"name": "小美", "age": 19, "gender": False, "height": 1.6, "weight": 45.0&#125;,]find_name = "阿土"for stu_dict in students: print(stu_dict) # 判断当前遍历的字典中姓名是否为find_name if stu_dict["name"] == find_name: print("找到了") # 如果已经找到，直接退出循环，就不需要再对后续的数据进行比较 breakelse: print("没有找到")print("循环结束")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>笔记 - Python - 学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gandiva Introspective Cluster Scheduling for Deep Learning]]></title>
    <url>%2F2018%2F12%2F07%2Fpaper_reading001%2F</url>
    <content type="text"><![CDATA[Gandiva: Introspective Cluster Scheduling for Deep Learning论文发表在OSDI’18会议上，是系统设计/实现方面的顶级会议。作者是微软的研究员们。 Abstract 深度学习的一个关键特征是反馈驱动探索，其中用户经常运行一组作业（或多作业）以搜索特定任务的最佳结果，并使用准确率等早期反馈来动态地优先化或结束一部分作业; 同时的早期反馈对整个多工作任务的进行至关重要。 第二个特征是深度学习工作在资源使用方面的异构性，难以实现最佳拟合。 Gandiva利用深度学习的第三个关键特征来解决这两个挑战：作业内可预测性，因为深度学习任务执行需要大量小批量重复迭代。 Gandiva利用作业内可预测性在多个作业中有效地对GPU进行时间分片，提供低延迟性。此可预测性还用于评估作业性能和动态迁移作业以更好地适应GPU，从而提高集群效率。 通过原型实现和微基准测试表明，Gandiva可以在深度学习期间将超参数搜索加速一个数量级，并通过透明迁移和时间切片从而实现更好的作业-资源匹配。实验表明，在180个GPU的集群中运行的实际工作负载中，Gandiva将聚合集群利用率提高了26％，表明这是一种管理大型GPU集群以进行深度学习的新方法。 1 Introduction深度学习是计算密集型的，严重依赖功能强大但价格昂贵的GPU;云中的GPU VM价格是普通VM的10倍。 云运营商和大公司依靠集群调度器来确保GPU的有效利用。 尽管高效调度深度学习训练（DLT）工作非常重要，但今天的常见做法是使用传统的集群调度程序，如Kubernetes或YARN这些用于处理大数据 MapReduce的工具; DLT作业被简单地视为大数据作业，在启动时分配一组GPU，并保持对其GPU的独占访问，直到完成为止。 DLT工作的一个关键特征是反馈驱动的探索（第2节）。由于深度学习实验固有的反复试验方法，用户通常会尝试几种任务的配置（多任务），并使用这些任务的早期反馈来决定是否优先考虑或结束它们的某些子集。这种称为超参数搜索的条件探索可以是手动的也可以是自动的。传统的调度程序在排队时运行一部分作业子集;这种模式不适合多任务，需要同时对多任务中的所有工作进行早期反馈。此外，与多任务一起，其他DLT作业已经确定了正确的超参数并运行了几个小时到几天，导致线头阻塞（长时间运行的作业可以独立访问GPU直到完成），而依赖在早期反馈的多任务仍排队等待。长队列时间迫使用户使用保留的GPU，或者要求群集过度配置，从而降低群集效率。 与任何其他群集工作负载一样，DLT作业是异构的，因为它们所针对的应用程序域不同。作业在内存使用，GPU核心利用率，带宽敏感度和其他作业的干扰性方面存在很大差异。例如，某些多GPU DLT作业可能对关联的GPU执行得更好，而其他作业可能对关联性不敏感（第3节）。将作业视为黑盒的传统调度程序因此将实现次优的集群效率。 为了解决高延迟和低效率这两个问题，Gandiva利用了DLT作业的强大属性：作业内可预测性（第3节）。一项任务由数百万个类似的，分开的小批量迭代组成。Gandiva利用这种循环可预测性来实现有效的应用感知时间切片; 它重新定义了从作业到自动划分的微任务的调度原子。这使集群能够超额预订DLT任务，并通过时间切片为所有DLT任务提供早期反馈（包括作为多任务一部分的所有作业）。 Gandiva还使用可预测性来执行简介驱动的内省。它使用小批量不断反省其决策，以提高集群效率（第4节）。例如，它只在内存和GPU利用率较低时才在同一GPU上打包多个作业; 它动态地将通信密集型作业迁移到更加接近的GPU上; 它还机会性地“增长”工作的并行度以利用备用资源，并在备用资源消失时缩减工作量。我们目前实施的内省策略是一种有状态的反复试验策略，由于我们考虑的可预测性和有限的选择状态空间，这是可行的。 除了本文评估的特定内省和调度策略之外，Gandiva框架还提供以下API，任何DLT调度策略都可以利用这些API：（a）有效的暂停 - 恢复或时间切片，（b）低延迟迁移，（c）细粒度分析，（d）动态的工作内弹性，以及（e）动态优先级。使这些原语高效和实用的关键是Gandiva的协同设计方法，跨越调度程序层和DLT工具包层，如Tensorflow或PyTorch。传统的调度程序，有充分的理由将工作视为一个黑盒子。 然而，通过利用GPU集群的专用特性，Gandiva将调度程序定制为深度学习的特定工作负载，从而为调度程序提供更多的可见性和对作业的控制，同时实现对任意DLT作业的通用性。 通过修改两个流行的框架PyTorch和Tensorflow来实现Gandiva，为调度程序提供必要的新原语，并在Kubernetes和Docker容器之上实现初始调度策略管理器（第5节）。 2 Background 反馈驱动的探索。实现高精度的一个先决条件是模型选择，像ResNet或Inception这样的模型通常是反复实验处理的过程，尽管自动化的方法是一个活跃的研究领域。除了模型结构之外，还有许多超参数需要指定为DLT作业的一部分。超参数包括模型中的层数/权重，批量大小，学习速率等。这些通常由用户根据领域知识和反复试验选择，有时甚至可能导致早期训练失败。 因此，DLT工作的早期反馈至关重要，特别是在训练的初始阶段。 多作业。 一旦用户识别出要进一步探索的特定模型，用户通常执行超参数搜索以提高任务准确性。这可以在超参数的空间上使用各种搜索技术来完成; 也就是说，用户生成多个DLT作业或多个作业，每个作业使用一组超参数或配置执行训练。由于用户通常会探索数百种此类配置，因此此过程的计算成本非常高。超参数搜索方法，如Hyper-Opt和Hyperband。Hyperband可能最初产生128个DLT作业，并且在每一轮（例如，100个小批量迭代）中，以最低精度结束一半的作业。对于这些算法，对工作的早期反馈至关重要，因为否则他们将无法做出有效的训练决策。 3 DLT Job Characteristics3.1 Sensitivity to locality多GPU DLT作业的性能取决于分配的GPU的亲和性。不同的DLT作业对GPU间亲和力表现出不同的灵敏度。即使对于同一台机器上的GPU，由于非对称架构，我们观察到不同级别的GPU间亲和性：两个GPU可能位于不同的CPU插槽中（表示为DiffSocket），位于同一CPU插槽中，但位于不同的PCIe交换机上（表示如同SameSocket），或在同一个PCIe交换机上（表示为SamePCIeSw）。 图1显示了VGG16和ResNet-50对服务器内局部性的不同敏感性。当使用Tensorflow对两个P100 GPU进行训练时，VGG16受到严重影响。当两个GPU位于不同的CPU插槽中时，VGG16仅实现最佳位置配置的60％，其中两个GPU放置在同一PCIe交换机下。另一方面，ResNet-50在此设置中不受GPU位置的影响。这是因为VGG16是一个比ResNet-50更大的神经模型，因此每个小批量的模型同步会在底层PCIe总线上产生更高的通信负载。 我们在分布式设置中观察到类似的趋势。显示了不同服务器间位置，训练ResNet-50和InceptionV3模型的4-GPU Tensorflow作业的性能。即使与40G InfiniBand网络互连，当作业分配到4个GPU时，也可以清楚地看到性能差异，它们均匀分散在4个服务器（表示为4 1-GPU），2个服务器（表示为2 2） -GPU），以及所有在一个服务器（表示为本地4-GPU），尽管两个模型的局部性的敏感性是不同的。因此，DLT调度程序在分配GPU时必须考虑作业对位置的敏感性。 3.2 Sensitivity to interference在共享执行环境中运行时，由于资源争用，DLT作业可能会相互干扰。我们再次观察到不同的DLT作业表现出不同程度的干扰。 对于单GPU作业也存在干扰。 将语言模型作业（标记为LM）与另一个作业放在同一PCI-e交换机下时，显示了由于服务器内干扰导致的性能下降。当两个LM一起运行时，两个工作都会减速19％。 但是，ResNet-50不会受到与LM共存的GPU的影响。 神经机器翻译（GNMT）表现出对LM的适度干扰。 显示了与40G InfiniBand网络连接的两台4 GPU服务器之间的服务器间干扰。 当运行多个2-GPU作业时，每个GPU放置在不同的服务器上，ResNet-50显示减速高达47％，InceptionV3显示减速30％，而DeepSpeech仅显示5％减速。 3.3 Intra-job predictabilityDLT作业包含许多小批量迭代。图5（a）中显示了在四个K80 GPU上使用ResNet-50模型时，在20秒的ImageNet数据训练期间使用的总GPU内存。所使用的GPU存储器明显遵循循环模式。这些循环中的每一个对应于单个小批量（约1.5s）的处理，其中存储器在正向传播期间增加并且在反向传播期间减小。使用的最大和最小GPU内存分别为23GB和0.3GB，为77倍。该比例与小批量大小成比例（通常在16到256之间;在这种情况下为128）。 在图5（b）中显示了在一个K80 GPU上使用GNMT模型时，在WMT’14 English German language数据集训练期间使用的总GPU内存。虽然小批量迭代在ImageNet示例中彼此不相同（由于不同的句子长度和PyTorch中动态图形的使用），图形具有类似的循环性质。最大值和最小值之间的差异较小（3x）主要是由于较大型号（0.4GB）和较小的小批量（本例中为16）。 除了此处显示的图像和语言模型之外，其他训练领域，如语音，GAN和变分自动编码器都遵循类似的循环模式（由于空间限制而未显示），因为训练的核心都是梯度下降算法执行许多小批量迭代。 利用可预测性。这种特征在Gandiva中以多种方式被利用。 首先，DLT作业可以自动拆分为小批量迭代，并且超过60秒的这些迭代的集合（微任务），形成调度间隔。其次，通过在存储器周期的最小值处执行挂起操作，可以显着减少要从GPU复制以保存在CPU中的存储量，从而使挂起/恢复和迁移能够比天真的实施更高效一个数量级。第三，可以对小批量进步率进行分析并将其用作代理，以评估应用包装或迁移等机制的有效性。 4 Design 由于DLT作业被分配了一组固定的GPU（图6），因此集群出现高延迟和低利用率。 对GPU的独占访问会导致行头阻塞；阻止早期反馈；导致传入作业的高排队时间。 当作业无法完全利用其分配的GPU时，对固定GPU的独占访问也会导致GPU利用率降低。 4.1 Mechanisms在Gandiva中，我们通过三种方式消除GPU对DLT作业的排他性和固定分配来解决这些低效问题（图6） Suspend-Resume and Packing。暂停 - 恢复是Gandiva用于删除一组GPU对DLT作业排他性的一种机制。现代操作系统支持CPU进程的高效挂起 - 恢复的时间切片。Gandiva利用这种机制并为GPU时间切片添加了自定义支持。如图5（a）所示，DLT作业对GPU内存的使用具有循环模式，最小和最大内存使用之间的差异高达77倍。 Gandiva的关键思想是利用这种循环行为，并在GPU内存使用率最低时采取暂停-恢复DLT作业。因此，当发出挂起调用时，DLT工具包会等待内存使用周期的最小值，将存储在GPU中的对象复制到CPU，释放所有GPU内存分配（包括缓存），然后调用经典CPU暂停机制。稍后，当CPU恢复作业时，DLT框架首先分配适当的GPU内存，将存储的对象复制回GPU，然后恢复作业。暂停 - 恢复还可以在同一服务器内改变GPU（例如，在六个1-GPU作业分时4-GPU的情况下）。 虽然更换GPU很昂贵，但我们会将这种延迟隐藏在关键路径之外。正如我们在评估（第6.1节）中所示，对于典型的图像分类工作，可以在100ms内完成暂停 - 恢复，而对于大型语言翻译工作，暂停 - 恢复可能需要1s。 给定1分钟的时间片间隔，这相当于2％或更少的开销。请注意，Gandiva中的暂停可能会延迟最多一个DLT作业的小批量间隔（通常为几秒或更短），但我们认为这是值得的权衡，因为它可以显着减少开销。 降低了GPU-CPU复制成本，减少了CPU中使用的内存。 此外，在此延迟期间完成了有用的工作。调度程序跟踪此延迟并相应地调整时间切片间隔以确保公平性。暂停 - 恢复时间切片的替代方法是同时在GPU上运行多个DLT作业，让GPU共享作业。我们称之为包装。 只有当打包作业不超过GPU资源（核心，内存）并且不会相互产生负面影响时，GPU中的打包才有效。 如果工作干扰，包装可能比暂停 - 恢复更糟糕（第6.1节）。 我们使用分析来监视DLT作业具有独占访问权限时的资源和进度。 如果两个工作被确定为包装的候选人，我们将它们打包在一起并继续监控它们。 如果给定的包装对工作效率产生不利影响，我们会拆开这些工作并返回暂停 - 恢复。 Migration。 迁移是Gandiva用于更改分配给DLT作业的GPU集合的机制。 迁移在以下几种情况下非常有用：i）将时间切片的作业移动到群集中腾出的任何位置的GPU;ii）将干扰工作相互迁移开;iii）群集的碎片化，以便传入的作业获得具有良好局部性的GPU。我们评估了两种解决DLT流程状态迁移的方法。（1）在第一种方法中，我们利用通用的流程迁移机制，如CRIU。因为CRIU本身不支持使用GPU设备的进程迁移，所以我们首先对GPU对象创建检查点并在调用CRIU之前从进程中删除所有GPU状态。 由于CRIU检查点并恢复整个进程内存，因此使用PyTorch检查点的大小为GB数量级。因此，对于单GPU作业，所产生的迁移开销约为8-10s，对于多GPU作业，则更高。（2）的第二种方法是使用支持检查点的DLT作业。诸如Tensorflow之类的DLT框架已经支持创建自动检查点和模型恢复的API（例如，tensorflow.train.saver）。此API现在用于确保不必因服务器故障而重新运行长时间运行的作业。我们扩展框架以支持此类工作的迁移。通过在迁移之前”预热“目的地并且仅迁移必要的训练状态，我们可以将迁移率减少到一秒或两秒（第6.1节）。无论采用哪种方法，我们都发现服务器间迁移的开销与其提供的更高整体GPU利用率相比是值得的。 Grow-Shrink。 Gandiva用于消除GPU对DLT作业的排他性的第三种机制是成长-缩减。该机制主要针对群集可能无法充分利用的情况，例如深夜时。基本思想是在空闲时机会性地增加可用于工作的GPU的数量，并且相应地减少负载增加时可用的GPU的数量。许多DLT作业（尤其是图像域中的作业）随着GPU数量的增加而看到线性性能缩放。Gandiva仅将这种机制应用于那些明确表明其足够自适应以利用这些增长机会的DLT工作。当多个DLT作业符合此标准时，Gandiva使用下面讨论的分析信息来估计每个作业的进度，然后相应地分配GPU。 Profiling。与任何调度程序一样，Gandiva监视资源使用情况，例如CPU和GPU利用率，CPU / GPU内存等。然而，Gandiva的独特之处在于它还以应用程序感知的方式内省DLT作业估计其进度。 这种内省利用了DLT作业（第3节）展示的规则模式，并使用周期性来估计其进度。Gandiva估计DLT作业的小批处理时间，即对一批输入数据进行一次前向/后向传递的时间，作为GPU内存使用周期的两个最小值之间所花费的时间（图5（a））。 由于DLT作业通常在其生命周期中执行数百万个这样的小批量操作，因此调度程序在调度决策之前和之后比较DLT的小批量时间以确定其有效性。例如，考虑在前面描述的GPU中打包两个DLT作业的示例。 通过比较包装前后两个DLT作业中每个作业的小批量时间，Gandiva可以决定包装是否有效。 如果没有这样的分析，为了做出包装决定，人们不仅要模拟两个DLT作业在各种GPU上的性能，还要模拟它们可能相互干扰的各种方式（例如，高速缓存，内存带宽等）。 。），这是一项非常重要的任务，我们在第6.1节中看到了不同的包装性能。 7 Related Work略 8 Conclusion略]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>深度学习任务调度 - 论文阅读 - 科研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习001_Pycharm常用知识]]></title>
    <url>%2F2018%2F12%2F05%2Fpython001%2F</url>
    <content type="text"><![CDATA[PyCharm 的初始设置目标 恢复 PyCharm 的初始设置 第一次启动 PyCharm 新建一个 Python 项目 设置 PyCharm 的字体显示 PyCharm 的升级以及其他 PyCharm 的官方网站地址是：https://www.jetbrains.com/pycharm/ 01. 恢复 PyCharm 的初始设置PyCharm 的 配置信息 是保存在 用户家目录下 的 .PyCharmxxxx.x 目录下的，xxxx.x 表示当前使用的 PyCharm 的版本号 如果要恢复 PyCharm 的初始设置，可以按照以下步骤进行： 关闭正在运行的 PyCharm 在终端中执行以下终端命令，删除 PyCharm 的配置信息目录： 1$ rm -r ~/.PyCharm2016.3 重新启动 PyCharm 02. 设置 PyCharm 的字体显示 03. PyCharm 的升级以及其他 PyCharm 提供了对 学生和教师免费使用的版本 教育版下载地址：https://www.jetbrains.com/pycharm-edu/download/#section=linux 专业版下载地址：https://www.jetbrains.com/pycharm/download/#section=linux 3.1 安装和启动步骤 执行以下终端命令，解压缩下载后的安装包 1$ tar -zxvf pycharm-professional-2017.1.3.tar.gz 将解压缩后的目录移动到 /opt 目录下，可以方便其他用户使用 /opt 目录用户存放给主机额外安装的软件 1$ sudo mv pycharm-2017.1.3/ /opt/ 切换工作目录 1$ cd /opt/pycharm-2017.1.3/bin 启动 PyCharm 1$ ./pycharm.sh 3.2 设置专业版启动图标 在专业版中，选择菜单 Tools / Create Desktop Entry… 可以设置任务栏启动图标 注意：设置图标时，需要勾选 Create the entry for all users 3.3 卸载之前版本的 PyCharm1) 程序安装 程序文件目录 将安装包解压缩，并且移动到 /opt 目录下 所有的相关文件都保存在解压缩的目录中 配置文件目录 启动 PyCharm 后，会在用户家目录下建立一个 .PyCharmxxx 的隐藏目录 保存 PyCharm 相关的配置信息 快捷方式文件 /usr/share/applications/jetbrains-pycharm.desktop 在 ubuntu 中，应用程序启动的快捷方式通常都保存在 /usr/share/applications 目录下 2) 程序卸载 要卸载 PyCharm 只需要做以下两步工作： 删除解压缩目录 1$ sudo rm -r /opt/pycharm-2016.3.1/ 删除家目录下用于保存配置信息的隐藏目录 1$ rm -r ~/.PyCharm2016.3/ 如果不再使用 PyCharm 还需要将 /usr/share/applications/ 下的 jetbrains-pycharm.desktop 删掉]]></content>
      <categories>
        <category>Python笔记</category>
      </categories>
      <tags>
        <tag>Python - 学习 - 笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令001]]></title>
    <url>%2F2018%2F12%2F04%2Flinux001%2F</url>
    <content type="text"><![CDATA[Linux终端命令的重要性 Linux 刚面世时并没有图形界面，所有的操作全靠命令完成，如 磁盘操作、文件存取、目录操作、进程管理、文件权限 设定等 在职场中，大量的 服务器维护工作 都是在 远程 通过 SSH 客户端 来完成的，并没有图形界面，所有的维护工作都需要通过命令来完成 在职场中，作为后端程序员，必须要或多或少的掌握一些 Linux 常用的终端命令 Linux 发行版本的命令大概有 200 多个，但是常用的命令只有 10 多个而已 学习终端命令的技巧： 不需要死记硬背，对于常用命令，用的多了，自然就记住了 不要尝试一次学会所有的命令，有些命令是非常不常用的，临时遇到，临时百度就可以 常用 Linux 命令的基本使用序号命令对应英文作用01lslist查看当前文件夹下的内容02pwdprint wrok directory查看当前所在文件夹03cd [目录名]change directory切换文件夹04touch [文件名]touch如果文件不存在，新建文件05mkdir [目录名]make directory创建目录06rm [文件名]remove删除指定的文件名07clearclear清屏 小技巧ctrl + shift + = 放大终端窗口的字体显示ctrl + - 缩小终端窗口的字体显示 自动补全在敲出 文件／目录／命令 的前几个字母之后，按下 tab 键如果输入的没有歧义，系统会自动补全如果还存在其他 文件／目录／命令，再按一下 tab 键，系统会提示可能存在的命令 小技巧按 上／下 光标键可以在曾经使用过的命令之间来回切换如果想要退出选择，并且不想执行当前选中的命令，可以按 ctrl + c]]></content>
      <categories>
        <category>Linux笔记</category>
      </categories>
      <tags>
        <tag>Linux - 学习 - 笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP1]]></title>
    <url>%2F2018%2F11%2F29%2FNLP1%2F</url>
    <content type="text"><![CDATA[CS224N自然语言处理自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。 作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词。形态分析（Morphological）是分析该词的构造形式（主要针对英语而言，例如，uninterested = un(前缀）+ interest(词干) + ed(后缀)）。句法分析是确定句子的语法结构句子中词汇之间的依存关系。本课程着重讲解句法分析和语义表示。 自然语言处理的应用 拼写检查、关键词检索…… 文本挖掘（产品价格、日期、时间、地点、人名、公司名） 文本分类 机器翻译 客服系统 复杂对话系统 在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人等。 人类语言的特殊之处与信号处理、数据挖掘不同，自然语言的随机性小而目的性强；语言是用来传输有意义的信息的，这种传输连小孩子都能很快学会。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性（赘余性）。所以说语言文字绝对不是形式逻辑或传统AI的产物。语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变。虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。 因为NVDIA官网即使是科学上网也经常崩，所以把百度网盘连接分享到文件里，需要的可以自取，密码ia9l 安装过程只要要三步： （1）下载并安装cuda9.1 （2）下载并安装cudnn7.1 (3) 下载并安装annaconda3-5.1.0 注意：1.查看cuda能否安装成功：在cmd(win+r)中输入：nvcc -V 2.安装cudnn7.0详细方法： 将cudnn压缩包中所有文件放入1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0 目录下对应目录中，同时将1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\extras\CUPTI\libx64\cupti64_91.dll 拷贝到1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin CUDA环境测试CMD中输入1nvcc -V 如果成功出现下图，则说明CUDA和Cudnn已经配置成功 安装tensorflow打开Anaconda Prompt，创建tensorflow虚拟环境1conda create -n tensorflow python=3.5 然后启用虚拟环境1activate tensorflow 最后安装tensorflow1pip install tensorflow-gpu 如果安装速度较慢，可以使用清华大学开源软件镜像站的TensorFlow 镜像。 测试tensorflow安装情况在刚才的Anaconda Prompt中输入1Python 在Python的交互界面中输入12345import tensorflow as tfhello=tf.constant(&apos;Hello tensorflow!&apos;)sess=tf.Session()sess.run(hello)sess.close() 然后激动人心的时刻，我们可以使用TF了。 可能出现的问题 找不到cudart64_90.dll: ImportError: DLL loaded failed: 找不到指定模块出现上述问题是显卡驱动没有更新，手动到NVDIA对应网站下载对应显卡最新驱动安装，即可。 额外内容为了方便之后的开发，我们可以安装Spyder这款十分强大的科学计算IDE。首先添加清华大学镜像源12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes 然后1conda install spyder 记得输入Yes，这样进行深度学习的环境就配置好了。 To be continue。。。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP - 学习 - 笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客正式上线]]></title>
    <url>%2F2018%2F09%2F20%2Ffirst%20blog%2F</url>
    <content type="text"><![CDATA[博客正式上线说了好久的博客现在终于用上了，准备记录一下自己研究生期间的所学和所思所想吧。也把个人博客作为展示自己的一个平台，为之后工作、科研做些积累。 A Good Programmer’s abilityA fool can write code that a computer can understand.Good programmers write code that humans can understand. Hexo tutorialsQuick startCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>随感</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>感想 - 日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow内核学习]]></title>
    <url>%2F2018%2F09%2F20%2Ftensor%2F</url>
    <content type="text"><![CDATA[Tensorflow内核探究1研究生期间要做的课题是深度学习inference任务的加速和调度问题，因为已经有很多不错的开源深度学习框架（Tensorflow，Pytorch2，Caffe2。。。），所以想在框架的基础上来做调度器和加速。然后Google大法确实厉害，有很多充足的文档、资料和论文实现，所以决定以Tensorflow入手，最近在学习Tensorflow的相关实现和原理，想在这篇博客里总结一下所学。 Tensorflow GPU版本安装要安装进行train，inference加速，我们需要一块好的GPU。如果你有NVDIA 1080 Ti或者NVDIA TitanX，那加速效果可能达到10——20倍，效果是很直观的。其他Nvdia显卡也可以，只是性能没有这么好。可以在NVDIA官网查看市面上常用显卡的计算性（使用Tensorflow GPU版本需要大于等于3.0以上的显卡）Nvdia显卡计算性 安装包准备安装的Tensorflow为V1.8版本，需要CUDA9.0和Cudnn7.1，推荐使用Anaconda来使用Tensorflow。anaconda3-5.1.0-windows10cuda-9.1-windows10cudnn-7.1-windows10关于Tensorflow和CUDA，Cudnn的版本对应关系可以参考下图。 Tensorflow版本 CUDA版本 Cudnn版本 Python版本 1.1-1.2 8.0 v5.1 3.5 1.3 8.0 v6，V6.1 3.5，3.6 1.4 8.0 V6.1 3.5，3.6 1.5-1.8 9.0 V7.0 3.5，3.6 因为NVDIA官网即使是科学上网也经常崩，所以把百度网盘连接分享到文件里，需要的可以自取，密码ia9l 安装过程只要要三步： （1）下载并安装cuda9.1 （2）下载并安装cudnn7.1 (3) 下载并安装annaconda3-5.1.0 注意：1.查看cuda能否安装成功：在cmd(win+r)中输入：nvcc -V 2.安装cudnn7.0详细方法： 将cudnn压缩包中所有文件放入1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0 目录下对应目录中，同时将1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\extras\CUPTI\libx64\cupti64_91.dll 拷贝到1C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin CUDA环境测试CMD中输入1nvcc -V 如果成功出现下图，则说明CUDA和Cudnn已经配置成功 安装tensorflow打开Anaconda Prompt，创建tensorflow虚拟环境1conda create -n tensorflow python=3.5 然后启用虚拟环境1activate tensorflow 最后安装tensorflow1pip install tensorflow-gpu 如果安装速度较慢，可以使用清华大学开源软件镜像站的TensorFlow 镜像。 测试tensorflow安装情况在刚才的Anaconda Prompt中输入1Python 在Python的交互界面中输入12345import tensorflow as tfhello=tf.constant(&apos;Hello tensorflow!&apos;)sess=tf.Session()sess.run(hello)sess.close() 然后激动人心的时刻，我们可以使用TF了。 可能出现的问题 找不到cudart64_90.dll: ImportError: DLL loaded failed: 找不到指定模块出现上述问题是显卡驱动没有更新，手动到NVDIA对应网站下载对应显卡最新驱动安装，即可。 额外内容为了方便之后的开发，我们可以安装Spyder这款十分强大的科学计算IDE。首先添加清华大学镜像源12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes 然后1conda install spyder 记得输入Yes，这样进行深度学习的环境就配置好了。 To be continue。。。]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>科研 - 学习</tag>
      </tags>
  </entry>
</search>
